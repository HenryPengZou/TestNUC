{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: ./../scripts_gpt4omini/output/nuc/banking77/banking77_pseudo_labeled_gpt-4o-mini.jsonl\n"
     ]
    }
   ],
   "source": [
    "## Parameters\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "dataset = 'banking77' # dataset: banking77, clinc, reddit, stackexchange, mtop_domain, clinc_domain, few_event, go_emotion, \n",
    "embedder = 'NV-Embed-v2' # 'NV-Embed-v2', 'gte-Qwen2-1.5B-instruct', 'stella-en-400M-v5', 'OpenAI'\n",
    "seed = 42\n",
    "\n",
    "\n",
    "llm = 'gpt-4o-mini' # 'gpt-4o-mini', 'gpt-4o', \"Meta-Llama-3.1-8B-Instruct-Turbo\", 'claude-3-haiku-20240307'\n",
    "llm_folder_name_map = {\n",
    "    'gpt-4o-mini': 'gpt4omini',\n",
    "    'gpt-4o': 'gpt4o',\n",
    "    \"Meta-Llama-3.1-8B-Instruct-Turbo\": 'llama31',\n",
    "    'claude-3-haiku-20240307': 'claude'}\n",
    "    \n",
    "# llm_folder_name = 'gpt4o' # gpt4o, gpt4omini, llama31, claude \n",
    "llm_folder_name = llm_folder_name_map[llm]\n",
    "\n",
    "data_path = f'./../scripts_{llm_folder_name}/output/nuc/{dataset}/{dataset}_pseudo_labeled_{llm}.jsonl'\n",
    "\n",
    "print('Data path:', data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 77\n",
      "Number of samples: 10003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>confidence</th>\n",
       "      <th>encoded_ground_truth</th>\n",
       "      <th>encoded_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My physical card is not working</td>\n",
       "      <td>card not working</td>\n",
       "      <td>card not working</td>\n",
       "      <td>0.95</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you unblock my account?  I entered the PIN...</td>\n",
       "      <td>pin blocked</td>\n",
       "      <td>pin blocked</td>\n",
       "      <td>0.90</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to know what is going on. I'm attemptin...</td>\n",
       "      <td>failed transfer</td>\n",
       "      <td>failed transfer</td>\n",
       "      <td>0.85</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am unable to prove my identity</td>\n",
       "      <td>unable to verify identity</td>\n",
       "      <td>unable to verify identity</td>\n",
       "      <td>0.92</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am still waiting for my card.</td>\n",
       "      <td>card arrival</td>\n",
       "      <td>card arrival</td>\n",
       "      <td>0.88</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>What is the present state of the exchange rate?</td>\n",
       "      <td>exchange rate</td>\n",
       "      <td>exchange rate</td>\n",
       "      <td>0.82</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>maximum how many days get the courier?</td>\n",
       "      <td>card arrival</td>\n",
       "      <td>card delivery estimate</td>\n",
       "      <td>0.76</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>How would I top up with a cheque?</td>\n",
       "      <td>top up by cash or cheque</td>\n",
       "      <td>top up by cash or cheque</td>\n",
       "      <td>0.85</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Where can I change my PIN?</td>\n",
       "      <td>change pin</td>\n",
       "      <td>change pin</td>\n",
       "      <td>0.90</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>I can't seem to figure out why my transfers ar...</td>\n",
       "      <td>declined transfer</td>\n",
       "      <td>declined transfer</td>\n",
       "      <td>0.95</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10003 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                        My physical card is not working   \n",
       "1      Can you unblock my account?  I entered the PIN...   \n",
       "2      I need to know what is going on. I'm attemptin...   \n",
       "3                       I am unable to prove my identity   \n",
       "4                        I am still waiting for my card.   \n",
       "...                                                  ...   \n",
       "9998     What is the present state of the exchange rate?   \n",
       "9999              maximum how many days get the courier?   \n",
       "10000                  How would I top up with a cheque?   \n",
       "10001                         Where can I change my PIN?   \n",
       "10002  I can't seem to figure out why my transfers ar...   \n",
       "\n",
       "                    ground_truth                 prediction  confidence  \\\n",
       "0               card not working           card not working        0.95   \n",
       "1                    pin blocked                pin blocked        0.90   \n",
       "2                failed transfer            failed transfer        0.85   \n",
       "3      unable to verify identity  unable to verify identity        0.92   \n",
       "4                   card arrival               card arrival        0.88   \n",
       "...                          ...                        ...         ...   \n",
       "9998               exchange rate              exchange rate        0.82   \n",
       "9999                card arrival     card delivery estimate        0.76   \n",
       "10000   top up by cash or cheque   top up by cash or cheque        0.85   \n",
       "10001                 change pin                 change pin        0.90   \n",
       "10002          declined transfer          declined transfer        0.95   \n",
       "\n",
       "       encoded_ground_truth  encoded_prediction  \n",
       "0                        17                  17  \n",
       "1                        58                  58  \n",
       "2                        38                  38  \n",
       "3                        79                  79  \n",
       "4                        14                  14  \n",
       "...                     ...                 ...  \n",
       "9998                     35                  35  \n",
       "9999                     14                  15  \n",
       "10000                    68                  68  \n",
       "10001                    24                  24  \n",
       "10002                    30                  30  \n",
       "\n",
       "[10003 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data \n",
    "\n",
    "# Load Pseudo-labeled Data\n",
    "data = pd.read_json(data_path, lines=True)\n",
    "\n",
    "# Obtain all unique labels\n",
    "labels = data['ground_truth'].unique()\n",
    "print('Number of unique labels:', len(labels))\n",
    "print('Number of samples:', len(data))\n",
    "\n",
    "# Rename column 'input' to 'text'\n",
    "data = data.rename(columns={'input': 'text'})\n",
    "\n",
    "# Encode the labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Combine unique values from both columns to create a consistent mapping\n",
    "unique_labels = pd.concat([data['ground_truth'], data['prediction']]).unique()\n",
    "\n",
    "# Fit the LabelEncoder on the combined unique labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "# Transform the columns using the same encoder\n",
    "data['encoded_ground_truth'] = label_encoder.transform(data['ground_truth'])\n",
    "data['encoded_prediction'] = label_encoder.transform(data['prediction'])\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Embeddings into the original data\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load original data\n",
    "data_path = f'../data/datasets/{dataset}/large.jsonl'\n",
    "data_original = pd.read_json(data_path, lines=True)\n",
    "# rename column 'input' to 'text'\n",
    "data_original = data_original.rename(columns={'input': 'text'})\n",
    "\n",
    "# only keep top 25k data\n",
    "data_original = data_original[:25000]\n",
    "\n",
    "print('Number of samples:', len(data_original))\n",
    "\n",
    "\n",
    "# Loading embeddings\n",
    "embedding_path = f'../data/embeddings/{dataset}/{embedder}.pt'\n",
    "\n",
    "data_embedding = torch.load(embedding_path)\n",
    "# as pandas dataframe\n",
    "data_embedding = pd.DataFrame(data_embedding.numpy())\n",
    "print(data_embedding.shape)\n",
    "\n",
    "\n",
    "# check if the embeddings shape is the same with with the original data\n",
    "assert data_embedding.shape[0] == len(data_original)\n",
    "\n",
    "# add the embeddings to the original data\n",
    "data_original['embedding'] = data_embedding.values.tolist()\n",
    "\n",
    "data_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes \n",
    "data_merge = data.merge(data_original, on=['text'], how='inner')\n",
    "\n",
    "# Only keep the necessary columns: text, ground_truth, prediction, confidence, encoded_ground_truth, encoded_prediction, and embeddings\n",
    "data_merge = data_merge[['text', 'ground_truth', 'prediction', 'confidence', 'encoded_ground_truth', 'encoded_prediction', 'embedding']]\n",
    "data_merge = data_merge.drop_duplicates(subset=['text', 'ground_truth', 'prediction', 'confidence', 'encoded_ground_truth', 'encoded_prediction'])\n",
    "\n",
    "# Obtain all unique labels\n",
    "labels = data_merge['ground_truth'].unique()\n",
    "print('Number of unique labels:', len(labels))\n",
    "print('Number of samples:', len(data_merge))\n",
    "\n",
    "# Display the first few rows of the data\n",
    "data = data_merge\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve neighbors and Aggregate neighborsâ€™ prediction with different number of unlabeled data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_unlabeled_data_list = [1, 500, 1000, 2500, 5000, 10000]\n",
    "\n",
    "best_results = []\n",
    "best_ks = []\n",
    "\n",
    "for num_unlabeled_data in num_unlabeled_data_list:\n",
    "    print('#' * 50)\n",
    "    print('Number of Unlabeled Data:', num_unlabeled_data)\n",
    "    # Data Setting: train, test = K, 10%\n",
    "    # num_unlabeled_data= 20000\n",
    "    data_train = data[:num_unlabeled_data]\n",
    "    data_test = data[int(0.9 * len(data)):]\n",
    "    print('Train, Test Split Size:', len(data_train), len(data_test))\n",
    "\n",
    "    # randomly select a subset of test data for evaluation\n",
    "    test_subset_size = 500\n",
    "    data_test_subset = data_test.sample(n=test_subset_size, random_state=seed).reset_index(drop=True)\n",
    "    # print('Test Subset Size:', len(data_test_subset))\n",
    "\n",
    "    # Read Test Embeddings\n",
    "    embeddings_test = data_test_subset['embedding'].apply(np.array)\n",
    "    embeddings_test = np.stack(embeddings_test).astype('float32')\n",
    "    # print('Test Embeddings shape:', embeddings_test.shape)\n",
    "    # Read Training Embeddings\n",
    "    embeddings_train = data_train['embedding'].apply(np.array)\n",
    "    embeddings_train = np.stack(embeddings_train).astype('float32')\n",
    "    # print('Training Embeddings shape:', embeddings_train.shape)\n",
    "\n",
    "    ## Mine Neighbors\n",
    "    # Configurations\n",
    "    topk = 500\n",
    "    k_list = [0, 1, 3, 5, 10, 20, 30, 40, 50, 60, 80, 100]\n",
    "\n",
    "    # FAISS Index\n",
    "    dim = embeddings_train.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings_train)\n",
    "    distances, indices = index.search(embeddings_test, topk)\n",
    "    # print('Indices shape:', indices.shape)\n",
    "\n",
    "    # Initialize statistics\n",
    "    majority_voting_accuracies = []\n",
    "    weighted_majority_accuracies = []\n",
    "    weighted_majority_accuracies_conf = []\n",
    "\n",
    "    # Loop through different k values\n",
    "    for k in k_list:\n",
    "        majority_accuracies = []\n",
    "        weighted_accuracies = []\n",
    "        weighted_accuracies_conf = []\n",
    "\n",
    "        for i in range(len(data_test_subset)):\n",
    "            # Retrieve neighbors\n",
    "            # neighbor_indices = indices[i, 1:1 + k]\n",
    "            neighbor_indices = indices[i, :k]\n",
    "            neighbor_labels = data_train.iloc[neighbor_indices]['encoded_prediction'].values\n",
    "            query_label = data_test_subset.iloc[i]['encoded_ground_truth']\n",
    "            # neighbor_distances = distances[i, 1:1 + k]\n",
    "            neighbor_distances = distances[i, :k]\n",
    "            neighbor_confidences = data_train.iloc[neighbor_indices]['confidence'].values\n",
    "\n",
    "            # add the query data itself to the neighbors\n",
    "            neighbor_labels = np.append(neighbor_labels, data_test_subset.iloc[i]['encoded_prediction'])\n",
    "            neighbor_distances = np.append(neighbor_distances, 1.0)\n",
    "            neighbor_confidences = np.append(neighbor_confidences, data_test_subset.iloc[i]['confidence'])\n",
    "            \n",
    "\n",
    "            # Majority Voting Accuracy\n",
    "            majority_label = pd.Series(neighbor_labels).mode().iloc[0]\n",
    "            majority_accuracies.append(int(majority_label == query_label))\n",
    "\n",
    "            # Weighted Majority Voting (based on distance)\n",
    "            # weights = 1 / (neighbor_distances + 1e-8)  # Avoid division by zero\n",
    "            weights = neighbor_distances  # Use distance/similarity as weight\n",
    "            weighted_votes = np.bincount(neighbor_labels, weights=weights)  # The function np.bincount() counts occurrences of each label in the neighbor_labels array, but with a twist: it allows you to assign a weight to each occurrence instead of treating all occurrences equally.\n",
    "            weighted_label = np.argmax(weighted_votes)\n",
    "            weighted_accuracies.append(int(weighted_label == query_label))\n",
    "\n",
    "            # Weighted Majority Voting (based on both confidence and distance)\n",
    "            weights_conf = neighbor_confidences * neighbor_distances\n",
    "            weighted_votes_conf = np.bincount(neighbor_labels, weights=weights_conf)\n",
    "            weighted_label_conf = np.argmax(weighted_votes_conf)\n",
    "            weighted_accuracies_conf.append(int(weighted_label_conf == query_label))\n",
    "\n",
    "        # Aggregate statistics\n",
    "        majority_voting_accuracies.append(np.mean(majority_accuracies))\n",
    "        weighted_majority_accuracies.append(np.mean(weighted_accuracies))\n",
    "        weighted_majority_accuracies_conf.append(np.mean(weighted_accuracies_conf))\n",
    "\n",
    "    # Plotting\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(k_list, majority_voting_accuracies, marker='s', label='Majority Voting Accuracy')\n",
    "    plt.plot(k_list, weighted_majority_accuracies, marker='^', label='Weighted Majority Voting Accuracy')\n",
    "    plt.plot(k_list, weighted_majority_accuracies_conf, marker='x', label='Weighted Majority Voting Accuracy (+Confidence)')\n",
    "\n",
    "    plt.xlabel('Number of Neighbors (k)')\n",
    "    plt.ylabel('Statistics')\n",
    "    plt.title(f'Neighbor-Based Evaluation with Different k on {dataset.upper()} Dataset')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # transform the results into a table with column being: k_list, with rows being, majority_voting_accuracies, weighted_majority_accuracies\n",
    "    results = np.array([majority_voting_accuracies, weighted_majority_accuracies, weighted_majority_accuracies_conf])\n",
    "    results = results.T * 100\n",
    "    results = pd.DataFrame(results, columns=['majority_voting_accuracies', 'weighted_majority_accuracies', 'weighted_majority_accuracies_conf'], index=k_list)\n",
    "    # add one more column for best result values among the last two columns\n",
    "    results['best_result'] = results[['majority_voting_accuracies', 'weighted_majority_accuracies', 'weighted_majority_accuracies_conf']].max(axis=1)\n",
    "    results.T\n",
    "\n",
    "    # only keep 5, 10, 20, 50, 100\n",
    "    results = results.loc[k_list]\n",
    "    results.T\n",
    "    print('Dataset Name:', dataset)\n",
    "    # print('Best Result:', results['best_result'].to_list())\n",
    "    print('Number of Unlabeld Data:', num_unlabeled_data)\n",
    "    print('Best k:', results['best_result'].idxmax(), 'Best Result:', results['best_result'].max())\n",
    "    # results\n",
    "    best_results.append(float(results['best_result'].max()))\n",
    "    best_ks.append(int(results['best_result'].idxmax()))\n",
    "\n",
    "print('#### Summary ####')\n",
    "print('Best Results:', best_results)\n",
    "print('Best k:', best_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in a dataframe\n",
    "results = pd.DataFrame({'num_unlabeled_data': num_unlabeled_data_list, 'best_results': best_results, 'best_ks': best_ks})\n",
    "results.to_csv(f'results.csv', float_format='%.3f')\n",
    "\n",
    "print(results)\n",
    "\n",
    "# plot the results\n",
    "plt.plot(results['num_unlabeled_data'], results['best_results'], marker='^', label=f'{llm}', alpha=1, lw=2.5, ms=7.5, color='green')\n",
    "\n",
    "plt.xlabel('Number of Unlabeled Data')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{dataset}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(f'results.pdf', format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestNUC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
